{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Buffer Memory\n",
    "\n",
    "ConversationBufferMemory keeps a list of chat messages in a buffer and passes those into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading legacy langchain libraries\n",
    "from langchain_core.prompts import (\n",
    "ChatPromptTemplate,\n",
    "MessagesPlaceholder,\n",
    "SystemMessagePromptTemplate, #system prompt\n",
    "HumanMessagePromptTemplate, #user prompt\n",
    ")\n",
    "from langchain.chains import LLMChain #legacy langchain chains\n",
    "from langchain.memory import ConversationBufferMemory ##legacy langchain memory\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        #system prompt\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        #whatever coversation we will be doing is called \"chat_history\"\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), #make sure you have to initialize this for memory \n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\") #human message prompt template\n",
    "    ]\n",
    ")\n",
    "\n",
    "#create a memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) #return_message - see the conversation\n",
    "\n",
    "#create a chain\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True, #see the execution log\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"hi\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"My name is Amar and I have moved 33 times.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"If my average moving distance was 100 miles, how many miles took all my moves?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversation Buffer Window Memory\n",
    "\n",
    "* Here we can mention the window limit to save like how many conversation we want to save.\n",
    "\n",
    "* Similar to previous one but you can limit the number of conversational exchange stored in memory.\n",
    "\n",
    "* For Example : you can set it like remember the last 3 question and respective answers of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_memory = ConversationBufferWindowMemory(k=3) #remembers only last 3 conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a chain\n",
    "conversation_window = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = window_memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give input\n",
    "conversation_window({\"input\": \"Hi, my name is Amar\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_window({\"input\": \"My favorite color is blue\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_window({\"input\": \"My favorite animals are dogs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_window({\"input\": \"I like to drive a vespa scooter in the city\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_window({\"input\": \"My favorite city is San Francisco\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_window({\"input\": \"What is my favorite color?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_window({\"input\": \"What is my favorite city?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chat Message History\n",
    "\n",
    "* Save the list of chat messages and then fetch them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
