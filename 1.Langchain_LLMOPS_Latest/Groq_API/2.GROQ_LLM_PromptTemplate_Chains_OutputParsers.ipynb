{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the API key\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "#openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llmModel = ChatGroq()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "ChatModel = ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts and Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, I'd be happy to share a curious story about APJ Abdul Kalam!\\n\\nOne day, when APJ Abdul Kalam was the President of India, he was scheduled to give a speech at a university. However, when he arrived at the university, he discovered that there was no stage or podium set up for him to speak from.\\n\\nUndeterred, Kalam simply walked over to a nearby tree and climbed up into its branches. From there, he delivered his entire speech to the assembled students and faculty.\\n\\nThe audience was initially surprised by Kalam's unconventional choice of speaking location, but they quickly became captivated by his words. Kalam spoke passionately about the importance of education and the power of science and technology to improve people's lives.\\n\\nAfter the speech, many students approached Kalam to ask him why he had chosen to speak from a tree. Kalam explained that he wanted to emphasize the importance of being adaptable and resourceful, even in the face of unexpected challenges. He also joked that the tree had provided a natural amplification system for his voice!\\n\\nThis curious story demonstrates Kalam's creativity, humility, and commitment to inspiring others. Even in a seemingly challenging situation, he was able to turn it into a memorable and impactful moment for all those present.\" response_metadata={'token_usage': {'completion_tokens': 287, 'prompt_tokens': 20, 'total_tokens': 307, 'completion_time': 0.460113861, 'prompt_time': 0.004904689, 'queue_time': 0.072118633, 'total_time': 0.46501855}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None} id='run-b1f35580-4e89-4cef-9472-61062c259fbb-0' usage_metadata={'input_tokens': 20, 'output_tokens': 287, 'total_tokens': 307}\n"
     ]
    }
   ],
   "source": [
    "# This is for completion models\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}\"\n",
    ")\n",
    "\n",
    "#pass the input to  the LLM model\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\",\n",
    "    topic=\"APJ Abdul Kalam\"\n",
    ")\n",
    "\n",
    "res = llmModel.invoke(llmModelPrompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rockfort temple, located in Tiruchirappalli, India, is believed to be over 2000 years old. The temple is built on a rock formation that is said to be over 3 billion years old. The exact age of the temple is not known, as there are no written records that date back to its construction. However, based on archaeological evidence and historical texts, it is estimated that the temple was built during the Pallava period, which dates back to the 6th century. Some parts of the temple complex, such as the Ucchi Pillayar Temple at the top of the rock, are believed to be even older, dating back to the 3rd century.\n"
     ]
    }
   ],
   "source": [
    "#Prompt template for chat completion model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #system Prompt\n",
    "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
    "        #user prompt\n",
    "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"), #Few short prompting\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    profession=\"History\",\n",
    "    topic=\"Rockfort temple\",\n",
    "    user_input=\"How old the rock fort temple is?\"\n",
    ")\n",
    "\n",
    "response = ChatModel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "[\n",
    "    SystemMessage(\n",
    "        content=(\n",
    "            \"You are an History expert on the Roger Federer.\"\n",
    "        )\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "user_input=\"Name his children and wife of Roger fedrer?\"\n",
    ")\n",
    "\n",
    "response = ChatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Roger Federer is married to Mirka Federer (née Mirka Vavrinec), a former professional tennis player from Slovakia. They have four children together: two sets of twins. Their older set of twins, girls, are Myla Rose Federer and Charlene Riva Federer, born in 2009. Their younger set of twins, boys, are Leo Federer and Lenny Federer, born in 2014. Mirka is often seen supporting Roger at his tennis matches and they make for a very supportive family unit.', response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 29, 'total_tokens': 151, 'completion_time': 0.193646579, 'prompt_time': 0.002651089, 'queue_time': 0.012502511, 'total_time': 0.196297668}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-0635b2e7-e7f0-42ef-991b-201684a0f1e5-0', usage_metadata={'input_tokens': 29, 'output_tokens': 122, 'total_tokens': 151})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roger Federer is married to Mirka Federer (née Mirka Vavrinec), a former professional tennis player from Slovakia. They have four children together: two sets of twins. Their older set of twins, girls, are Myla Rose Federer and Charlene Riva Federer, born in 2009. Their younger set of twins, boys, are Leo Federer and Lenny Federer, born in 2014. Mirka is often seen supporting Roger at his tennis matches and they make for a very supportive family unit.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompting Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "#Language Translation - if the user is giving english sentence then convert into spanish\n",
    "#If we have more than one prompts then it comes under few shot prompting\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "#Chat prompt template\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Prepare template\n",
    "#inside the fewshot template we need to pass the prompt and then chat prompt template\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "#Lets create the final prompt\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡hola! ¿En qué puedo ayudarte hoy? (hi! How can I help you today?)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "#Language Translation - if the user is giving english sentence then convert into spanish\n",
    "#If we have more than one prompts then it comes under few shot prompting\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "#Chat prompt template\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Prepare template\n",
    "#inside the fewshot template we need to pass the prompt and example\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "#Lets create the final prompt\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Latest Langchain syntax for chains\n",
    "#first we need to give the prompt | pipe symbol - then it goes to the large language model\n",
    "#here how are you sentence go to the LLM , then llm gives one output, then i format the output using output parser(text, json)\n",
    "chain = final_prompt | ChatModel\n",
    "res = chain.invoke({\"input\": \"hi!\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡hola! ¿En qué puedo ayudarte hoy? (hi! How can I help you today?)', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 50, 'total_tokens': 76, 'completion_time': 0.039813703, 'prompt_time': 0.003705006, 'queue_time': 0.010794614, 'total_time': 0.043518709}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-99678dcc-2b0b-47e2-9c09-74b111193ded-0', usage_metadata={'input_tokens': 50, 'output_tokens': 26, 'total_tokens': 76})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain = final_prompt | chatModel - > This type of syntax is called Langchain Expression Language(LECL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llmModel = ChatGroq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "ChatModel = ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "json_chain = json_prompt | llmModel | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res  = json_chain.invoke({\"question\": \"What is the biggest country?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The biggest country in the world is Russia, which covers approximately 17.1 million square kilometers.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize the output with the help of output parser based on our requirements\n",
    "### Pydantic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic Object with the desired output format.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parser referring the Pydantic Object\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Add the parser format instructions in the prompt definition.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create a chain with the prompt and the parser\n",
    "chain = prompt | ChatModel | parser\n",
    "\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
